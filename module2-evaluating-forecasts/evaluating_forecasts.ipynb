{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluating_forecasts",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efr4aqm9opfd",
        "colab_type": "text"
      },
      "source": [
        "_Lambda School Data Science — Regression 2_\n",
        "\n",
        "# Evaluating Forecasts\n",
        "\n",
        "#### Objectives\n",
        "- explain why overfitting is a problem and model validation is important\n",
        "- do train/test split\n",
        "- use time series metrics: MSE, RMSE, MAE, MAPE\n",
        "- do time series cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3rywcXcAwm6",
        "colab_type": "text"
      },
      "source": [
        "## Example solutions for stretch goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "601MJfX0izJz",
        "colab_type": "text"
      },
      "source": [
        "### Use the Wikimedia Pageviews API to get data\n",
        "- Get good at learning new things by googling and reading documentation. Jake Vander Plas calls this [\"the real world bootcamp\"](https://twitter.com/jakevdp/status/648593367786323968).\n",
        "- To learn how to get data from an API, follow along with the [Requests library quickstart](https://2.python-requests.org/en/master/user/quickstart/), or [_Automate the Boring Stuff with Python_, Chapter 14](https://automatetheboringstuff.com/chapter14/) by Al Swiegart.\n",
        "- Then, refer to the [Wikipedia Pageviews API quickstart](https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews#Quick_start) and [documentation](https://wikimedia.org/api/rest_v1/#/Pageviews%20data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8M73YZ-BPT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "project = 'en.wikipedia'\n",
        "access = 'all-access'\n",
        "agent = 'user'\n",
        "article = 'Veganism'\n",
        "granularity = 'daily'\n",
        "start = '20150701'\n",
        "end = '20190527'\n",
        "\n",
        "endpoint = f'/metrics/pageviews/per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
        "url = 'https://wikimedia.org/api/rest_v1' + endpoint\n",
        "response = requests.get(url)\n",
        "assert response.status_code == 200\n",
        "df = pd.DataFrame(response.json()['items'])\n",
        "df = df.rename(columns={'timestamp':'ds', 'views':'y'})\n",
        "df['ds'] = pd.to_datetime(df['ds'], format='%Y%m%d%H')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddmspiF2i6ZL",
        "colab_type": "text"
      },
      "source": [
        "### Adjust your forecasts with Prophet's changepoints and holidays options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWFvYzWoks7j",
        "colab_type": "text"
      },
      "source": [
        "#### Prophet documentation: [Trend Changepoints](https://facebook.github.io/prophet/docs/trend_changepoints.html)\n",
        "\n",
        "> Real time series frequently have abrupt changes in their trajectories. By default, Prophet will automatically detect these changepoints and will allow the trend to adapt appropriately. However, if you wish to have finer control over this process (e.g., Prophet missed a rate change, or is overfitting rate changes in the history), then there are several input arguments you can use.\n",
        "\n",
        "#### Prophet documentation: [Holiday Effects](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html)\n",
        "\n",
        "> If you have holidays or other recurring events that you’d like to model, you must create a dataframe for them. It has two columns (`holiday` and `ds`) and a row for each occurrence of the holiday. It must include all occurrences of the holiday, both in the past (back as far as the historical data go) and in the future (out as far as the forecast is being made). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPY43h83k4Gr",
        "colab_type": "text"
      },
      "source": [
        "#### Model holidays, plot changepoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g49vZGpcLSFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fbprophet import Prophet\n",
        "from fbprophet.plot import add_changepoints_to_plot\n",
        "\n",
        "holidays = pd.DataFrame({\n",
        "  'holiday': 'new_years',\n",
        "  'ds': pd.to_datetime(['2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', \n",
        "                        '2019-01-01, 2020-01-01', '2021-01-01'], utc=True),\n",
        "  'lower_window': -5,\n",
        "  'upper_window': 5,\n",
        "})\n",
        "\n",
        "model = Prophet(daily_seasonality=False, holidays=holidays)\n",
        "model.fit(df)\n",
        "future = model.make_future_dataframe(periods=365*2)\n",
        "forecast = model.predict(future)\n",
        "fig1 = model.plot(forecast)\n",
        "add_changepoints_to_plot(fig1.gca(), model, forecast)\n",
        "fig2 = model.plot_components(forecast)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22QNZOM7kn5x",
        "colab_type": "text"
      },
      "source": [
        "#### Will Koehrsen's blog post, [Time Series Analysis in Python](https://towardsdatascience.com/time-series-analysis-in-python-an-introduction-70d5a5b1d52a)\n",
        "\n",
        "> When creating the prophet models, I set the changepoint prior to 0.15, up from the default value of 0.05. This hyperparameter is used to control [how sensitive the trend is to changes](https://facebook.github.io/prophet/docs/trend_changepoints.html), with a higher value being more sensitive and a lower value less sensitive. This value is used to combat one of the most fundamental trade-offs in machine learning: [bias vs. variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).\n",
        "\n",
        "> If we fit too closely to our training data, called [overfitting](https://elitedatascience.com/overfitting-in-machine-learning), we have too much variance and our model will not be able to generalize well to new data. On the other hand, if our model does not capture the trends in our training data it is underfitting and has too much bias. When a model is underfitting, increasing the changepoint prior allows more flexibility for the model to fit the data, and if the model is overfitting, decreasing the prior limits the amount of flexibility. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7eo7EvelfWb",
        "colab_type": "text"
      },
      "source": [
        "#### Adjust the `changepoint_prior_scale` parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs6GM-7pIhby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scales = [0.01, 0.05, 0.15, 0.50]\n",
        "for scale in scales:\n",
        "    model = Prophet(daily_seasonality=False, holidays=holidays, \n",
        "                    changepoint_prior_scale=scale)\n",
        "    model.fit(df)\n",
        "    future = model.make_future_dataframe(periods=365*2)\n",
        "    forecast = model.predict(future)\n",
        "    fig1 = model.plot(forecast)\n",
        "    add_changepoints_to_plot(fig1.gca(), model, forecast)\n",
        "    plt.title(f'changepoint_prior_scale={scale}')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXK4CruVljus",
        "colab_type": "text"
      },
      "source": [
        "***How can we know if our model is good? How should we adjust the right parameters?***\n",
        "\n",
        "*We do model validation!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEvtr62Ub39u",
        "colab_type": "text"
      },
      "source": [
        "## Explain why overfitting is a problem and model validation is important"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQizSBsgqkLY",
        "colab_type": "text"
      },
      "source": [
        "#### Jason Brownlee, [Overfitting and Underfitting With Machine Learning Algorithms](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)\n",
        "\n",
        "> The goal of a good machine learning model is to **generalize** well from the training data to any data from the problem domain. This allows us to make predictions in the future on data the model has never seen.\n",
        "\n",
        "> The cause of poor performance in machine learning is either overfitting or underfitting the data.\n",
        "\n",
        "> **Overfitting** refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. \n",
        "\n",
        "> **Underfitting** refers to a model that can neither model the training data nor generalize to new data.\n",
        "\n",
        "> Ideally, you want to select a model at the sweet spot between underfitting and overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zah7byfYaM7C",
        "colab_type": "text"
      },
      "source": [
        "#### Rob Hyndman & George Athanasopoulos, [_Forecasting: Principles and Practice_, Chapter 3.4](https://otexts.com/fpp2/accuracy.html), Evaluating forecast accuracy:\n",
        "\n",
        "> The following points should be noted.\n",
        "\n",
        "> - A model which fits the training data well will not necessarily forecast well.\n",
        "> - A perfect fit can always be obtained by using a model with enough parameters.\n",
        "> - Over-fitting a model to data is just as bad as failing to identify a systematic pattern in the data.\n",
        "\n",
        "> **The accuracy of forecasts can only be determined by considering how well a model performs on new data that were not used when fitting the model.**\n",
        "\n",
        "> When choosing models, it is common practice to separate the available data into two portions, training and test data, where the training data is used to estimate any parameters of a forecasting method and the test data is used to evaluate its accuracy. Because the test data is not used in determining the forecasts, it should provide a reliable indication of how well the model is likely to forecast on new data.\n",
        "\n",
        "![](https://otexts.com/fpp2/fpp_files/figure-html/traintest-1.png)\n",
        "\n",
        "> The size of the test set is typically about 20% of the total sample, although this value depends on how long the sample is and how far ahead you want to forecast. The test set should ideally be at least as large as the maximum forecast horizon required.\n",
        "\n",
        "> Some references describe the test set as the “hold-out set” because these data are “held out” of the data used for fitting. Other references call the training set the “in-sample data” and the test set the “out-of-sample data”. We prefer to use “training data” and “test data” in this book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyxLs3s_ozp0",
        "colab_type": "text"
      },
      "source": [
        "#### James, Witten, Hastie, Tibshirani, [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), Chapter 2.2, Assessing Model Accuracy\n",
        "\n",
        "> In general, we do not really care how well the method works training on the training data. Rather, _we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data._ Why is this what we care about? \n",
        "\n",
        "> Suppose that we are interested test data in developing an algorithm to predict a stock’s price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don’t really care how well our method predicts last week’s stock price. We instead care about how well it will predict tomorrow’s price or next month’s price. \n",
        "\n",
        "> On a similar note, suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for _future patients_ based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes.\n",
        "\n",
        "#### Rachel Thomas, [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/)\n",
        "\n",
        "> An all-too-common scenario: a seemingly impressive machine learning model is a complete failure when implemented in production. The fallout includes leaders who are now skeptical of machine learning and reluctant to try it again. How can this happen?\n",
        "\n",
        "> One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). \n",
        "\n",
        "#### Owen Zhang, [Winning Data Science Competitions](https://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions/8)\n",
        "\n",
        "> Good validation is _more important_ than good models. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnScYMsCAN5E",
        "colab_type": "text"
      },
      "source": [
        "## Do train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAcdcayimQG6",
        "colab_type": "text"
      },
      "source": [
        "### Read weather data\n",
        "- This was another stretch goal from the previous notebook: You can [get daily weather station data](https://www.ncdc.noaa.gov/cdo-web/search) from the NOAA (National Oceanic and Atmospheric Administration) for your local area. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIl6mGJpdol7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Regression-1/master/module2-evaluating-forecasts/weather-normal-il.csv'\n",
        "weather = (pd.read_csv(url, parse_dates=['DATE'])\n",
        "             .rename(columns={'DATE':'ds', 'TMAX':'y'}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKFcqbXZmoAA",
        "colab_type": "text"
      },
      "source": [
        "### Split the weather data: train with 2010-17, and test with 2018-19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFsTXLSXvxt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjyVkHhGnRIH",
        "colab_type": "text"
      },
      "source": [
        "### Plot daily high temperatures for train (blue) and test (red)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDxczfR4nMCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCKiUVSGxgdp",
        "colab_type": "text"
      },
      "source": [
        "## Begin with baselines for time series\n",
        "\n",
        "### Mean Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1ktvHIoxf-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "defsVx4vy-ty",
        "colab_type": "text"
      },
      "source": [
        "### Naive Baseline (Last Observation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6twFtfVzBJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX4Dwxad5FQM",
        "colab_type": "text"
      },
      "source": [
        "## Use time series metric: MAE (Mean Absolute Error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-rKqAe04O7B",
        "colab_type": "text"
      },
      "source": [
        "#### Rob Hyndman & George Athanasopoulos, [_Forecasting: Principles and Practice_, Chapter 3.4](https://otexts.com/fpp2/accuracy.html), Evaluating forecast accuracy:\n",
        "\n",
        "> A forecast “error” is the difference between an observed value and its forecast.\n",
        "\n",
        "> We can measure forecast accuracy by summarising the forecast errors in different ways.\n",
        "\n",
        "> The two most commonly used scale-dependent measures are based on the absolute errors or squared errors:\n",
        "\n",
        "> Mean absolute error: $\\mathrm{MAE}=\\operatorname{mean}\\left(\\left|e_{t}\\right|\\right)$\n",
        "\n",
        "> Root mean squared error: $\\mathrm{RMSE}=\\sqrt{\\operatorname{mean}\\left(e_{t}^{2}\\right)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEBFEkFqx-gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDITeBq3wXgZ",
        "colab_type": "text"
      },
      "source": [
        "## Use Prophet to forecast time series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ7LHl4QuLMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeTzbH7w8fZ0",
        "colab_type": "text"
      },
      "source": [
        "## Use time series metric: MAE (Mean Absolute Error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zBZCoa3uF25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UILj2dvdARae",
        "colab_type": "text"
      },
      "source": [
        "## Do time series cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQdLI7u76LrA",
        "colab_type": "text"
      },
      "source": [
        "#### Rob Hyndman & George Athanasopoulos, [_Forecasting: Principles and Practice_, Chapter 3.4](https://otexts.com/fpp2/accuracy.html), Evaluating forecast accuracy:\n",
        "\n",
        "> A more sophisticated version of training/test sets is time series cross-validation. In this procedure, there are a series of test sets ... The corresponding training set consists only of observations that occurred _prior_ to the test set. \n",
        "\n",
        "> The following diagram illustrates the series of training and test sets, where the blue observations form the training sets, and the red observations form the test sets.\n",
        "\n",
        "![](https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png)\n",
        "\n",
        "> The forecast accuracy is computed by averaging over the test sets. This procedure is sometimes known as “evaluation on a rolling forecasting origin” because the “origin” at which the forecast is based rolls forward in time.\n",
        "\n",
        "> With time series forecasting, one-step forecasts may not be as relevant as multi-step forecasts. In this case, the cross-validation procedure can be modified. Suppose that we are interested in models that produce good  4-step-ahead forecasts. Then the corresponding diagram is shown below.\n",
        "\n",
        "![](https://otexts.com/fpp2/fpp_files/figure-html/cv4-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz9Dztzr9Chg",
        "colab_type": "text"
      },
      "source": [
        "#### Prophet documentation: [Diagnostics](https://facebook.github.io/prophet/docs/diagnostics.html)\n",
        "\n",
        ">Prophet includes functionality for time series cross validation to measure forecast error using historical data. This is done by selecting cutoff points in the history, and for each of them fitting the model using data only up to that cutoff point. We can then compare the forecasted values to the actual values. \n",
        "\n",
        ">This figure illustrates a simulated historical forecast on the Peyton Manning dataset, where the model was fit to a initial history of 5 years, and a forecast was made on a one year horizon.\n",
        "\n",
        "![](https://facebook.github.io/prophet/static/diagnostics_files/diagnostics_3_0.png)\n",
        "\n",
        "> This cross validation procedure can be done automatically for a range of historical cutoffs using the `cross_validation` function. We specify the forecast horizon (`horizon`), and then optionally the size of the initial training period (`initial`) and the spacing between cutoff dates (`period`). \n",
        "\n",
        "> The output of `cross_validation` is a dataframe with the true values `y` and the out-of-sample forecast values `yhat`, at each simulated forecast date and for each cutoff date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhm3fcXjAUax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpYqwAvP_2F3",
        "colab_type": "text"
      },
      "source": [
        "## Use time series metrics: MSE, RMSE, MAE, MAPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbQlFWGD_8Ii",
        "colab_type": "text"
      },
      "source": [
        "#### Prophet documentation: [Diagnostics](https://facebook.github.io/prophet/docs/diagnostics.html)\n",
        "\n",
        "> The `performance_metrics` utility can be used to compute some useful statistics of the prediction performance ... The statistics computed are mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), mean absolute percent error (MAPE), and coverage of the `yhat_lower` and `yhat_upper` estimates. These are computed on a rolling window of the predictions in `df_cv` after sorting by horizon ... By default 10% of the predictions will be included in each window, but this can be changed with the `rolling_window` argument.\n",
        "\n",
        "> Cross validation performance metrics can be visualized with `plot_cross_validation_metric`, here shown for MAPE. Dots show the absolute percent error for each prediction in `df_cv`. The blue line shows the MAPE, where the mean is taken over a rolling window of the dots. We see for this forecast that errors around 5% are typical for predictions one month into the future, and that errors increase up to around 11% for predictions that are a year out.\n",
        "\n",
        "\n",
        "![](https://facebook.github.io/prophet/static/diagnostics_files/diagnostics_12_0.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAVRtUsO-kwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lOMc0NRAWFb",
        "colab_type": "text"
      },
      "source": [
        "## Assignment\n",
        "\n",
        "- Use the same data you chose for the previous lesson, or get new data.\n",
        "- Do train/test split. Report your mean absolute error on the train and test set, for these forecasts:\n",
        "  - Mean Baseline\n",
        "  - Naive Baseline (Last Observation)\n",
        "  - Prophet forecast (you can choose the parameters or use the defaults)\n",
        "- Do time series cross-validation, using these Prophet functions:\n",
        "  - cross_validation\n",
        "  - performance_metrics\n",
        "  - plot_cross_validation_metric\n",
        "- Commit your notebook to your fork of the GitHub repo.\n",
        "\n",
        "### Stretch Challenges\n",
        "- **Share your visualizations on Slack!**\n",
        "- Use the Wikimedia Pageviews API to get data.\n",
        "- [Get daily weather station data](https://www.ncdc.noaa.gov/cdo-web/search) from the NOAA (National Oceanic and Atmospheric Administration). User Prophet to forecast the weather for your local area. \n",
        "- Adjust your forecasts with Prophet's [changepoints](https://facebook.github.io/prophet/docs/trend_changepoints.html) and [holidays](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html) options. In addition to Prophet's documentation, read Will Koehrsen's blog post, [Time Series Analysis in Python](https://towardsdatascience.com/time-series-analysis-in-python-an-introduction-70d5a5b1d52a). **How do these parameters affect your error metrics?**\n",
        "- Learn more about how Prophet works. Read the [tweestorm with animated GIFs](https://twitter.com/seanjtaylor/status/1123278380369973248) by Prophet developer Sean J. Taylor, or his [research paper](https://peerj.com/preprints/3190/)."
      ]
    }
  ]
}